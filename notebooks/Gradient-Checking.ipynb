{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Gradient Checking\n",
    "</h1><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's so easy to have bugs/errors in our implementation of back-propagation. Therefore, it's necessary before running the neural network on training data to check if our back-propagation implementation is correct. Before we start, let's revisit what back-propagation is. With back-propagation, we loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge's node tail. In other words, we compute the the derivative of cost function with respect to all parameters, i.e $\\frac{\\partial J}{\\partial \\theta}$ where $\\theta$ represents the parameters of the model.\n",
    "\n",
    "We'll test our implementation by computing numerical gradients and compare it with gradients from back-propagation (analytical). Also, we'll use two-sided epsilon method to compute the numerical gradients as follows:\n",
    "$$\\frac{\\partial J}{\\partial \\theta} = \\lim_{\\epsilon \\rightarrow 0}\\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2 \\epsilon}$$ Where is the difference can be computed using the following formula:\n",
    "$$\\frac{\\|grad - grad_{approx}\\|_2}{\\|grad\\|_2 + \\|grad_{approx}\\|_2}$$\n",
    "If the difference is $\\leq 10^{-7}$, then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit back-propagation code.\n",
    "\n",
    "Below are the steps to implement gradient checking:\n",
    "1. Pick random number of examples from training data to use it when computing both numerical and analytical gradients.\n",
    "    - Don't use all examples in the training data because gradient checking is very slow.\n",
    "2. Initialize parameters.\n",
    "3. Compute forward propagation and the cross-entropy cost.\n",
    "4. Compute the gradients using our back-propagation implementation.\n",
    "5. Compute the numerical gradients using the two-sided epsilon method.\n",
    "6. Compute the difference between numerical and analytical gradients.\n",
    "\n",
    "Note that we'll use functions we wrote in *\"Coding Deep Neural Network from Scratch\"* notebook to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost. Let's first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import os as os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original shape: (100, 150, 150, 3)\n",
      "The transformed shape: (67500, 100)\n"
     ]
    }
   ],
   "source": [
    "# set up the path\n",
    "os.chdir(\"../data/\")\n",
    "\n",
    "# get all file names to iterate over all of them\n",
    "image_list_names = os.listdir()[1:]\n",
    "\n",
    "# loading images\n",
    "X = []\n",
    "\n",
    "for img in image_list_names:\n",
    "    if img.startswith(\".\"):\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        temp = np.array(plt.imread(img))\n",
    "        X.append(temp)\n",
    "        \n",
    "# convert to numpy array\n",
    "X = np.array(X)\n",
    "print(\"The original shape: {}\".format(X.shape))\n",
    "\n",
    "# Derive true label vector\n",
    "Y = np.zeros((1, 100))\n",
    "\n",
    "for i, img in enumerate(image_list_names):\n",
    "    if img.startswith(\"cat\"):\n",
    "        Y[:, i] = 1\n",
    "        \n",
    "    elif img.startswith(\"dog\"):\n",
    "        Y[:, i] = 0\n",
    "        \n",
    "# reshape X\n",
    "num_pix = X.shape[1]\n",
    "m = X.shape[0]\n",
    "X = X.reshape(m, -1).T\n",
    "print(\"The transformed shape: {}\".format(X.shape))\n",
    "\n",
    "# standarize the data\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we'll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries. After that, we'll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if the our implementation of back-propagation is correct. We'll randomly choose 3 examples to compute the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all helper functions needed\n",
    "os.chdir(\"../scripts/\")\n",
    "from coding_deep_neural_network_from_scratch import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dictionary_to_vector(dictionary):\n",
    "    \"\"\"\n",
    "    Roll all dictionary into a single vector.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    for key in dictionary.keys():\n",
    "        new_vector = np.reshape(dictionary[key], (-1, 1))\n",
    "\n",
    "        if count == 0:\n",
    "            theta_vector = new_vector\n",
    "\n",
    "        else:\n",
    "            theta_vector = np.concatenate((theta_vector, new_vector), axis=0)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return theta_vector\n",
    "\n",
    "\n",
    "def vector_to_dictionary(vector, layers_dims):\n",
    "    \"\"\"\n",
    "    Unroll parameters vector to dictionary using layers dimensions.\n",
    "\n",
    "    Arguments:\n",
    "    vector -- parameters vector\n",
    "    layers_dims -- list or numpy array that has the dimensions of each layer\n",
    "                    in the network.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing all parameters\n",
    "    \"\"\"\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    k = 0\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # create temp variable to store dimension used on each layer\n",
    "        w_dim = layers_dims[l] * layers_dims[l - 1]\n",
    "        b_dim = layers_dims[l]\n",
    "\n",
    "        # create temporary var to be used in slicing parameters vector\n",
    "        temp_dim = k + w_dim\n",
    "\n",
    "        # add parameters to the dictionary\n",
    "        parameters[\"W\" + str(l)] = vector[\n",
    "            k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])\n",
    "        parameters[\"b\" + str(l)] = vector[\n",
    "            temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n",
    "\n",
    "        k += w_dim + b_dim\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all gradients into a single vector containing only dW and db\n",
    "    \"\"\"\n",
    "    # get the number of indices for the gradients to iterate over\n",
    "    valid_grads = [key for key in gradients.keys()\n",
    "                   if not key.startswith(\"dA\")]\n",
    "    L = len(valid_grads)// 2\n",
    "    count = 0\n",
    "    \n",
    "    # iterate over all gradients and append them to new_grads list\n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        if count == 0:\n",
    "            new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n",
    "            new_grads = np.concatenate(\n",
    "                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)), axis=0)\n",
    "        \n",
    "        else:\n",
    "            new_grads = np.concatenate(\n",
    "                (new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)), axis=0)\n",
    "            new_grads = np.concatenate(\n",
    "                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)), axis=0)\n",
    "    \n",
    "        count += 1\n",
    "        \n",
    "    return new_grads\n",
    "\n",
    "\n",
    "def forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn = \"tanh\"):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation and computes the cost.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of shape number of features x number of examples.\n",
    "    parameters -- python dictionary containing all parameters.\n",
    "    Y -- true \"label\" of shape 1 x number of examples.\n",
    "    hidden_layers_activation_fn -- activation function to be used on hidden\n",
    "                                   layers,string: \"tanh\", \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost.\n",
    "    \"\"\"\n",
    "    # compute forward prop\n",
    "    AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n",
    "\n",
    "    # compute cost\n",
    "    cost = compute_cost(AL, Y)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def gradient_check_n(\n",
    "        parameters, gradients, X, Y, layers_dims, epsilon = 1e-7,\n",
    "        hidden_layers_activation_fn=\"tanh\"):\n",
    "    \"\"\"\n",
    "    Checks if back_prop computes correctly the gradient of the cost output by\n",
    "    forward_prop.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing all parameters.\n",
    "    gradients -- output of back_prop, contains gradients of the cost ww.r.t\n",
    "    the parameters. \n",
    "    X -- input data of shape number of features x number of examples.\n",
    "    Y -- true \"label\" of shape 1 x number of examples.\n",
    "    epsilon -- tiny shift to the input to compute approximate gradient\n",
    "    layers_dims -- list or numpy array that has the dimensions of each layer\n",
    "                   in the network.\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference between approx gradient and back_prop gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # roll out parameters and gradients dictionaries\n",
    "    parameters_vector = dictionary_to_vector(parameters)\n",
    "    gradients_vector = gradients_to_vector(gradients)\n",
    "\n",
    "    # create vector of zeros to be used with epsilon\n",
    "    grads_approx = np.zeros_like(parameters_vector)\n",
    "\n",
    "    for i in range(len(parameters_vector)):\n",
    "        # compute cost of theta + epsilon\n",
    "        theta_plus = np.copy(parameters_vector)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        j_plus = forward_prop_cost(\n",
    "            X, vector_to_dictionary(theta_plus, layers_dims), Y,\n",
    "            hidden_layers_activation_fn)\n",
    "\n",
    "        # compute cost of theta - epsilon\n",
    "        theta_minus = np.copy(parameters_vector)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        j_minus = forward_prop_cost(\n",
    "            X, vector_to_dictionary(theta_minus, layers_dims), Y,\n",
    "            hidden_layers_activation_fn)\n",
    "\n",
    "        # compute numerical gradients\n",
    "        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n",
    "\n",
    "    # compute the difference of numerical and analytical gradients\n",
    "    numerator = np.linalg.norm(gradients_vector - grads_approx)\n",
    "    denominator = np.linalg.norm(grads_approx) +\\\n",
    "    np.linalg.norm(gradients_vector)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 10e-7:\n",
    "        print (\"\\033[31m\" + \"There is a mistake in back-propagation\",\n",
    "               \"implementation. The difference is: {}\".format(difference))\n",
    "    \n",
    "    else:\n",
    "        print (\"\\033[32m\" + \"There implementation of back-propagation is fine!\",\n",
    "               \"The difference is: {}\".format(difference))\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThere implementation of back-propagation is fine! The difference is: 9.715560673355063e-07\n"
     ]
    }
   ],
   "source": [
    "# set up neural network architecture\n",
    "layers_dims = [X.shape[0], 5, 5, 1]\n",
    "\n",
    "# initialize parameters\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "# randomly selection 5 examples from training data\n",
    "perms = np.random.permutation(X.shape[1])\n",
    "inex = perms[:3]\n",
    "\n",
    "# compute forward propagation\n",
    "AL, caches = L_model_forward(X[:, inex], parameters, \"tanh\")\n",
    "\n",
    "# compute analytical gradients\n",
    "gradients = L_model_backward(AL, Y[:, inex], caches, \"tanh\")\n",
    "\n",
    "# compute difference of numerical and analytical gradients\n",
    "difference = gradient_check_n(parameters, gradients, X[:, inex], Y[:, inex], layers_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! our implementation is correct :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since gradient checking is very slow:\n",
    "    - apply it on one or few training examples.\n",
    "    - turn it off when training neural network after making sure that back-propagation's implementation is correct.\n",
    "- gradient checking doesn't work when applying drop-out method. Use keep-prob = 1 to check gradient checking and then change it when training neural network.\n",
    "- epsilon = 10e-7 is common value used for the difference between analytical gradient and numerical gradient. If the difference is less than 10e-7 then the implementation of back-prop is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
