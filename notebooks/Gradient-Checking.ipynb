{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Gradient Checking\n",
    "</h1><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import os as os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original shape: (100, 150, 150, 3)\n",
      "The transformed shape: (67500, 100)\n"
     ]
    }
   ],
   "source": [
    "# set up the path\n",
    "os.chdir(\"../data/\")\n",
    "\n",
    "# get all file names to iterate over all of them\n",
    "image_list_names = os.listdir()[1:]\n",
    "\n",
    "# loading images\n",
    "X = []\n",
    "\n",
    "for img in image_list_names:\n",
    "    if img.startswith(\".\"):\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        temp = np.array(plt.imread(img))\n",
    "        X.append(temp)\n",
    "        \n",
    "# convert to numpy array\n",
    "X = np.array(X)\n",
    "print(\"The original shape: {}\".format(X.shape))\n",
    "\n",
    "# Derive true label vector\n",
    "Y = np.zeros((1, 100))\n",
    "\n",
    "for i, img in enumerate(image_list_names):\n",
    "    if img.startswith(\"cat\"):\n",
    "        Y[:, i] = 1\n",
    "        \n",
    "    elif img.startswith(\"dog\"):\n",
    "        Y[:, i] = 0\n",
    "        \n",
    "# reshape X\n",
    "num_pix = X.shape[1]\n",
    "m = X.shape[0]\n",
    "X = X.reshape(m, -1).T\n",
    "print(\"The transformed shape: {}\".format(X.shape))\n",
    "\n",
    "# standarize the data\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all helper functions needed\n",
    "os.chdir(\"../scripts/\")\n",
    "from coding_deep_neural_network_from_scratch import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dictionary_to_vector(dictionary):\n",
    "    \"\"\"\n",
    "    Roll all dictionary into a single vector.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    for key in dictionary.keys():\n",
    "        new_vector = np.reshape(dictionary[key], (-1, 1))\n",
    "\n",
    "        if count == 0:\n",
    "            theta_vector = new_vector\n",
    "\n",
    "        else:\n",
    "            theta_vector = np.concatenate((theta_vector, new_vector), axis=0)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return theta_vector\n",
    "\n",
    "\n",
    "def vector_to_dictionary(vector, layers_dims):\n",
    "    \"\"\"\n",
    "    Unroll parameters vector to dictionary using layers dimensions.\n",
    "\n",
    "    Arguments:\n",
    "    vector -- parameters vector\n",
    "    layers_dims -- list or numpy array that has the dimensions of each layer\n",
    "                    in the network.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing all parameters\n",
    "    \"\"\"\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    k = 0\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # create temp variable to store dimension used on each layer\n",
    "        w_dim = layers_dims[l] * layers_dims[l - 1]\n",
    "        b_dim = layers_dims[l]\n",
    "\n",
    "        # create temporary var to be used in slicing parameters vector\n",
    "        temp_dim = k + w_dim\n",
    "\n",
    "        # add parameters to the dictionary\n",
    "        parameters[\"W\" + str(l)] = vector[k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])\n",
    "        parameters[\"b\" + str(l)] = vector[temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n",
    "\n",
    "        k += w_dim + b_dim\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all gradients into a single vector containing only dW and db\n",
    "    \"\"\"\n",
    "    # get the number of indices for the gradients to iterate over\n",
    "    valid_grads = [key for key in gradients.keys()\n",
    "                   if not key.startswith(\"dA\")]\n",
    "    L = len(valid_grads)// 2\n",
    "    count = 0\n",
    "    \n",
    "    # iterate over all gradients and append them to new_grads list\n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        if count == 0:\n",
    "            new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n",
    "            new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)), axis=0)\n",
    "        \n",
    "        else:\n",
    "            new_grads = np.concatenate((new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)), axis=0)\n",
    "            new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)), axis=0)\n",
    "    \n",
    "        count += 1\n",
    "        \n",
    "    return new_grads\n",
    "\n",
    "\n",
    "def forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn = \"tanh\"):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation and computes the cost.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of shape number of features x number of examples.\n",
    "    parameters -- python dictionary containing all parameters.\n",
    "    Y -- true \"label\" of shape 1 x number of examples.\n",
    "    hidden_layers_activation_fn -- activation function to be used on hidden\n",
    "                                   layers,string: \"tanh\", \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost.\n",
    "    \"\"\"\n",
    "    # compute forward prop\n",
    "    AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n",
    "\n",
    "    # compute cost\n",
    "    cost = compute_cost(AL, Y)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, layers_dims, epsilon = 1e-7, hidden_layers_activation_fn=\"tanh\"):\n",
    "    \"\"\"\n",
    "    Checks if back_prop computes correctly the gradient of the cost output by\n",
    "    forward_prop.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing all parameters.\n",
    "    gradients -- output of back_prop, contains gradients of the cost ww.r.t\n",
    "    the parameters. \n",
    "    X -- input data of shape number of features x number of examples.\n",
    "    Y -- true \"label\" of shape 1 x number of examples.\n",
    "    epsilon -- tiny shift to the input to compute approximate gradient\n",
    "    layers_dims -- list or numpy array that has the dimensions of each layer\n",
    "                   in the network.\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference between approx gradient and back_prop gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # roll out parameters and gradients dictionaries\n",
    "    parameters_vector = dictionary_to_vector(parameters)\n",
    "    gradients_vector = gradients_to_vector(gradients)\n",
    "\n",
    "    # create vector of zeros to be used with epsilon\n",
    "    j_plus = np.zeros_like(parameters_vector)\n",
    "    j_minus = np.zeros_like(parameters_vector)\n",
    "    grads_approx = np.zeros_like(parameters_vector)\n",
    "\n",
    "    for i in range(len(parameters_vector)):\n",
    "        # compute cost of theta + epsilon\n",
    "        theta_plus = np.copy(parameters_vector)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        j_plus = forward_prop_cost(X, vector_to_dictionary(theta_plus, layers_dims), Y, hidden_layers_activation_fn)\n",
    "\n",
    "        # compute cost of theta - epsilon\n",
    "        theta_minus = np.copy(parameters_vector)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        j_minus = forward_prop_cost(X, vector_to_dictionary(theta_minus, layers_dims), Y, hidden_layers_activation_fn)\n",
    "\n",
    "        # compute numerical gradients\n",
    "        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n",
    "\n",
    "    # compute the difference of numerical and analytical gradients\n",
    "    numerator = np.linalg.norm(gradients_vector - grads_approx)\n",
    "    denominator = np.linalg.norm(grads_approx) + np.linalg.norm(gradients_vector)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 10e-7:\n",
    "        print (\"\\033[91m\" + \"There is a mistake in back-propagation\",\n",
    "               \"implementation. The difference is: {}\".format(difference))\n",
    "    \n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"There implementation of back-propagation is fine!\"\n",
    "               \"The difference is: {}\".format(difference))\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-bd037e2473a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# compute difference of numerical and analytical gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_check_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-646f0b49b7d8>\u001b[0m in \u001b[0;36mgradient_check_n\u001b[0;34m(parameters, gradients, X, Y, layers_dims, epsilon, hidden_layers_activation_fn)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtheta_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mtheta_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mj_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_to_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers_activation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# compute cost of theta - epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-646f0b49b7d8>\u001b[0m in \u001b[0;36mforward_prop_cost\u001b[0;34m(X, parameters, Y, hidden_layers_activation_fn)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \"\"\"\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# compute forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers_activation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# compute cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/imad/Documents/Deep-Learning/Deep-Learning-Coursera/scripts/coding_deep_neural_network_from_scratch.py\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters, hidden_layers_activation_fn)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layers_activation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/imad/Documents/Deep-Learning/Deep-Learning-Coursera/scripts/coding_deep_neural_network_from_scratch.py\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation_fn)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation_fn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/imad/Documents/Deep-Learning/Deep-Learning-Coursera/scripts/coding_deep_neural_network_from_scratch.py\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(A_prev, W, b)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mstores\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \"\"\"\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set up neural network architecture\n",
    "layers_dims = [X.shape[0], 5, 5, 1]\n",
    "\n",
    "# initialize parameters\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "# compute forward propagation\n",
    "AL, caches = L_model_forward(X, parameters, \"tanh\")\n",
    "\n",
    "# compute analytical gradients\n",
    "gradients = L_model_backward(AL, Y, caches, \"tanh\")\n",
    "\n",
    "# compute difference of numerical and analytical gradients\n",
    "difference = gradient_check_n(parameters, gradients, X, Y, layers_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
